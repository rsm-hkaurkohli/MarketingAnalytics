[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hargun Kaur Kohli",
    "section": "",
    "text": "I am a Master’s student in Business Analytics at UC San Diego, with a focus on customer and marketing analytics. I have hands-on experience in digital marketing, product analytics, and business analysis through internships at The Brand Blueprint Podcast, Ernst and Young, and Govind Milk and Milk Products. Currently, I lead a capstone project developing anomaly detection models for 340B demand forecasting in the pharmaceutical sector. Proficient in Python, SQL, Tableau, and Power BI, I am keen to apply my analytical skills to drive data-driven business solutions"
  },
  {
    "objectID": "hw2_questions.html",
    "href": "hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "hw2_questions.html#blueprinty-case-study",
    "href": "hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "hw2_questions.html#airbnb-case-study",
    "href": "hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided."
  },
  {
    "objectID": "projects/Project 1/MAA2.html",
    "href": "projects/Project 1/MAA2.html",
    "title": "1 DATA",
    "section": "",
    "text": "# Importing required libraries\nimport pandas as pd\n\n# Load the Blueprinty dataset\ndata = pd.read_csv('blueprinty.csv')\n\n# Inspect the first few rows\ndata.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0"
  },
  {
    "objectID": "projects/Project 1/MAA2.html#estimation-of-simple-poisson-model",
    "href": "projects/Project 1/MAA2.html#estimation-of-simple-poisson-model",
    "title": "1 DATA",
    "section": "2. Estimation of Simple Poisson Model",
    "text": "2. Estimation of Simple Poisson Model\n\nWrite down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\n\n\n\nimage.png\n\n\n\n\nCode the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:_\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\n\nimport numpy as np\nfrom scipy.special import gammaln  # for stable log(Y!) computation\n\n# Define the Poisson log-likelihood function\ndef poisson_loglikelihood(lam, Y):\n    \"\"\"\n    Computes the negative log-likelihood for Poisson model.\n\n    Parameters:\n    lam (float): Lambda parameter of the Poisson distribution\n    Y (array-like): Array of observed count data (e.g., patents)\n\n    Returns:\n    float: Negative log-likelihood (for use with optimizers)\n    \"\"\"\n    if lam &lt;= 0:\n        return np.inf  # to handle invalid (non-positive) lambda\n\n    n = len(Y)\n    log_likelihood = -n * lam + np.sum(Y * np.log(lam)) - np.sum(gammaln(Y + 1))\n    return -log_likelihood  # return negative because optimizers minimize\n\n\n\n\nUse your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y)._\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\n\n# Define the Poisson log-likelihood function\ndef poisson_loglikelihood(lam, Y):\n    if lam &lt;= 0:\n        return np.inf\n    n = len(Y)\n    log_likelihood = -n * lam + np.sum(Y * np.log(lam)) - np.sum(gammaln(Y + 1))\n    return -log_likelihood  # for minimization\n\n# Load data\nimport pandas as pd\ndata = pd.read_csv('blueprinty.csv')\nY = data['patents'].values\n\n# Generate lambda values and evaluate log-likelihood\nlambda_values = np.linspace(0.1, 10, 200)\nlog_likelihoods = [-poisson_loglikelihood(lam, Y) for lam in lambda_values]\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.plot(lambda_values, log_likelihoods, color='blue')\nplt.axvline(x=np.mean(Y), color='red', linestyle='--', label='Sample Mean (MLE)')\nplt.title('Log-Likelihood of Poisson Model')\nplt.xlabel('Lambda (λ)')\nplt.ylabel('Log-Likelihood')\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/Project 1/MAA2.html#if-youre-feeling-mathematical-take-the-first-derivative-of-your-likelihood-or-log-likelihood-set-it-equal-to-zero-and-solve-for-lambda.-you-will-find-lambda_mle-is-ybar-which-feels-right-because-the-mean-of-a-poisson-distribution-is-lambda._",
    "href": "projects/Project 1/MAA2.html#if-youre-feeling-mathematical-take-the-first-derivative-of-your-likelihood-or-log-likelihood-set-it-equal-to-zero-and-solve-for-lambda.-you-will-find-lambda_mle-is-ybar-which-feels-right-because-the-mean-of-a-poisson-distribution-is-lambda._",
    "title": "1 DATA",
    "section": "If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda._",
    "text": "If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda._\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\nFind the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\n\n# Load data\ndata = pd.read_csv('blueprinty.csv')\nY = data['patents'].values\n\n# Define the negative log-likelihood function for optimization\ndef poisson_neg_loglikelihood(lam, Y):\n    lam = lam[0]  # unpack scalar from array\n    if lam &lt;= 0:\n        return np.inf\n    n = len(Y)\n    log_likelihood = -n * lam + np.sum(Y * np.log(lam)) - np.sum(gammaln(Y + 1))\n    return -log_likelihood  # return negative for minimization\n\n# Initial guess: sample mean\nlambda_start = [np.mean(Y)]\n\n# Perform optimization\nresult = minimize(poisson_neg_loglikelihood, x0=lambda_start, args=(Y,), bounds=[(1e-6, None)])\nlambda_mle = result.x[0]\n\n# Print result\nprint(f\"MLE for lambda: {lambda_mle:.4f}\")\n\nMLE for lambda: 3.6847"
  },
  {
    "objectID": "projects/Project 1/MAA2.html#estimation-of-poisson-regression-model",
    "href": "projects/Project 1/MAA2.html#estimation-of-poisson-regression-model",
    "title": "1 DATA",
    "section": "3. Estimation of Poisson Regression Model",
    "text": "3. Estimation of Poisson Regression Model\n\nUpdate your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that_ \\(\\lambda_i = e^{X_i'\\beta}\\). _For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\n\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_regression_neg_loglikelihood(beta, Y, X):\n    \"\"\"\n    Negative log-likelihood function for Poisson regression.\n\n    Parameters:\n    beta (array-like): Coefficient vector (including intercept).\n    Y (array-like): Response variable (count data).\n    X (2D array): Design matrix with covariates (must include intercept column).\n\n    Returns:\n    float: Negative log-likelihood value for optimization.\n    \"\"\"\n    # Linear predictor\n    eta = np.dot(X, beta)\n    # Apply exponential inverse link function to get lambda_i\n    lam = np.exp(eta)\n    \n    # Log-likelihood computation\n    log_likelihood = np.sum(Y * np.log(lam) - lam - gammaln(Y + 1))\n    return -log_likelihood  # Negative for minimization\n\n\n\nUse your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\nfrom scipy.special import gammaln\n\n# Load the data\ndata = pd.read_csv('blueprinty.csv')  # Replace with correct path if needed\nY = data['patents'].to_numpy(dtype=np.float64)\n\n# Create covariates\ndata['age_sq'] = data['age'] ** 2\nregion_dummies = pd.get_dummies(data['region'], prefix='region', drop_first=True)\n\n# Design matrix with intercept\nX = pd.concat([\n    pd.DataFrame({'const': 1}, index=data.index),\n    data[['age', 'age_sq', 'iscustomer']],\n    region_dummies\n], axis=1)\n\nX_mat = X.to_numpy(dtype=np.float64)\n\n# Poisson negative log-likelihood function\ndef poisson_regression_neg_loglikelihood(beta, Y, X):\n    beta = np.asarray(beta, dtype=np.float64)\n    eta = X @ beta\n    lam = np.exp(eta)  # Safe use of np.exp on np.ndarray\n    ll = np.sum(Y * np.log(lam) - lam - gammaln(Y + 1))\n    return -ll  # Negative for minimization\n\n# Initial guess\nbeta_start = np.zeros(X_mat.shape[1])\n\n# Run optimization\nresult = minimize(\n    poisson_regression_neg_loglikelihood,\n    beta_start,\n    args=(Y, X_mat),\n    method='BFGS'\n)\n\n# Extract results\nbeta_mle = result.x\nhessian_inv = result.hess_inv\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\n# Display coefficients and standard errors\nresults_df = pd.DataFrame({\n    'Coefficient': beta_mle,\n    'Std. Error': std_errors\n}, index=X.columns)\n\nprint(\"\\nPoisson Regression Coefficients and Standard Errors:\\n\")\nprint(results_df)\n\n\nPoisson Regression Coefficients and Standard Errors:\n\n                  Coefficient  Std. Error\nconst                1.480059         1.0\nage                 38.016417         1.0\nage_sq            1033.539585         1.0\niscustomer           0.553874         1.0\nregion_Northeast     0.640979         1.0\nregion_Northwest     0.164288         1.0\nregion_South         0.181562         1.0\nregion_Southwest     0.295497         1.0\n\n\n/tmp/ipykernel_30792/1831057527.py:27: RuntimeWarning: overflow encountered in exp\n  lam = np.exp(eta)  # Safe use of np.exp on np.ndarray\n/tmp/ipykernel_30792/1831057527.py:28: RuntimeWarning: invalid value encountered in multiply\n  ll = np.sum(Y * np.log(lam) - lam - gammaln(Y + 1))\n/tmp/ipykernel_30792/1831057527.py:28: RuntimeWarning: invalid value encountered in subtract\n  ll = np.sum(Y * np.log(lam) - lam - gammaln(Y + 1))\n/opt/conda/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n/tmp/ipykernel_30792/1831057527.py:27: RuntimeWarning: overflow encountered in exp\n  lam = np.exp(eta)  # Safe use of np.exp on np.ndarray\n/tmp/ipykernel_30792/1831057527.py:28: RuntimeWarning: invalid value encountered in multiply\n  ll = np.sum(Y * np.log(lam) - lam - gammaln(Y + 1))\n/tmp/ipykernel_30792/1831057527.py:28: RuntimeWarning: invalid value encountered in subtract\n  ll = np.sum(Y * np.log(lam) - lam - gammaln(Y + 1))\n/opt/conda/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n/tmp/ipykernel_30792/1831057527.py:27: RuntimeWarning: overflow encountered in exp\n  lam = np.exp(eta)  # Safe use of np.exp on np.ndarray\n/tmp/ipykernel_30792/1831057527.py:28: RuntimeWarning: invalid value encountered in multiply\n  ll = np.sum(Y * np.log(lam) - lam - gammaln(Y + 1))\n/tmp/ipykernel_30792/1831057527.py:28: RuntimeWarning: invalid value encountered in subtract\n  ll = np.sum(Y * np.log(lam) - lam - gammaln(Y + 1))\n\n\n\n\nCheck your results using R’s glm() function or Python sm.GLM() function.\n\nimport pandas as pd\nimport statsmodels.api as sm\n\n# Load data\ndata = pd.read_csv('blueprinty.csv')  # Replace with your correct path\n\n# Create age squared\ndata['age_sq'] = data['age'] ** 2\n\n# Create region dummies (drop the first to avoid multicollinearity)\nregion_dummies = pd.get_dummies(data['region'], prefix='region', drop_first=True)\n\n# Combine all predictors\nX = pd.concat([\n    data[['age', 'age_sq', 'iscustomer']],\n    region_dummies\n], axis=1)\n\n# Add constant column for intercept\nX = sm.add_constant(X)\n\n# Ensure all data is float64 (fix for ValueError)\nX = X.astype(float)\nY = data['patents'].astype(float)\n\n# Fit Poisson regression model\nmodel = sm.GLM(Y, X, family=sm.families.Poisson()).fit()\n\n# Show coefficient summary table\nprint(model.summary2().tables[1])\n\n                     Coef.  Std.Err.          z         P&gt;|z|    [0.025  \\\nconst            -0.508920  0.183179  -2.778269  5.464935e-03 -0.867944   \nage               0.148619  0.013869  10.716250  8.539597e-27  0.121438   \nage_sq           -0.002970  0.000258 -11.513237  1.131496e-30 -0.003476   \niscustomer        0.207591  0.030895   6.719179  1.827509e-11  0.147037   \nregion_Northeast  0.029170  0.043625   0.668647  5.037205e-01 -0.056334   \nregion_Northwest -0.017575  0.053781  -0.326782  7.438327e-01 -0.122983   \nregion_South      0.056561  0.052662   1.074036  2.828066e-01 -0.046655   \nregion_Southwest  0.050576  0.047198   1.071568  2.839141e-01 -0.041931   \n\n                    0.975]  \nconst            -0.149896  \nage               0.175801  \nage_sq           -0.002465  \niscustomer        0.268144  \nregion_Northeast  0.114674  \nregion_Northwest  0.087833  \nregion_South      0.159778  \nregion_Southwest  0.143083  \n\n\n\n\nInterpret the results.\nInterpretation of Poisson Regression Results The Poisson regression model estimates the number of patents awarded to engineering firms as a function of several covariates, including firm age, age squared, regional location, and whether the firm is a customer of Blueprinty. The key findings are as follows:\nBlueprinty Customer Status: The coefficient for the iscustomer variable is 0.208 and statistically significant at the 1% level (p &lt; 0.001). This implies that, holding all other variables constant, firms using Blueprinty’s software experience a higher expected count of patents. Exponentiating the coefficient yields an incidence rate ratio of exp ⁡ ( 0.208 ) ≈ 1.23 exp(0.208)≈1.23, indicating that customers of Blueprinty are expected to receive approximately 23% more patents over the five-year period than non-customers. This provides empirical support for Blueprinty’s marketing claim.\nFirm Age and Age Squared: The model includes both age and age squared to capture non-linear effects. The coefficient on age is 0.149 (p &lt; 0.001), suggesting that the number of patents increases with firm age. However, the coefficient on age squared is -0.003 (p &lt; 0.001), indicating diminishing returns to age. Together, these results imply an inverted U-shaped relationship: patent success rates rise with firm age initially but taper off or decline for very mature firms.\nRegional Effects: Dummy variables for regional location were included, with one region omitted as the reference group. None of the regional coefficients were statistically significant at conventional levels (p &gt; 0.05), suggesting that regional location does not materially influence patent counts once age and software usage are controlled for.\nConclusion The results of the Poisson regression model provide strong evidence that Blueprinty’s software is positively associated with a higher rate of patent awards. Firm age is also a significant predictor, though its effect diminishes at higher levels. The analysis finds no compelling evidence that regional variation plays a significant role. These findings support the use of Blueprinty’s product among engineering firms seeking to enhance their patent success rates.\n\n\nWhat do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences.\n\n\n# Extract coefficients and standard errors\nparams = model.params\nconf = model.conf_int()\nconf.columns = ['2.5%', '97.5%']\n\n# Compute IRR and CI by exponentiating\nirr = np.exp(params)\nirr_conf = np.exp(conf)\n\n# Combine into a summary table\nirr_summary = pd.DataFrame({\n    'IRR': irr,\n    'CI Lower (2.5%)': irr_conf['2.5%'],\n    'CI Upper (97.5%)': irr_conf['97.5%']\n})\n\n# Display\nprint(\"\\nIncidence Rate Ratios (IRR) with 95% Confidence Intervals:\\n\")\nprint(irr_summary)\n\n\nIncidence Rate Ratios (IRR) with 95% Confidence Intervals:\n\n                       IRR  CI Lower (2.5%)  CI Upper (97.5%)\nconst             0.601145         0.419814          0.860797\nage               1.160231         1.129119          1.192201\nage_sq            0.997034         0.996530          0.997538\niscustomer        1.230709         1.158397          1.307536\nregion_Northeast  1.029600         0.945223          1.121508\nregion_Northwest  0.982579         0.884279          1.091806\nregion_South      1.058191         0.954417          1.173250\nregion_Southwest  1.051877         0.958936          1.153825"
  },
  {
    "objectID": "projects/Project 1/MAA2.html#drop-rows-with-missing-values-in-relevant-variables",
    "href": "projects/Project 1/MAA2.html#drop-rows-with-missing-values-in-relevant-variables",
    "title": "1 DATA",
    "section": "# Drop rows with missing values in relevant variables",
    "text": "# Drop rows with missing values in relevant variables\nrelevant_vars = [ ‘number_of_reviews’, ‘days’, ‘room_type’, ‘bathrooms’, ‘bedrooms’, ‘price’, ‘review_scores_cleanliness’, ‘review_scores_location’, ‘review_scores_value’, ‘instant_bookable’] airbnb_clean = airbnb_df[relevant_vars].dropna()"
  },
  {
    "objectID": "projects/Project 1/MAA2.html#a-poisson-regression-model-was-used-to-analyze-how-airbnb-listing-characteristics-affect-the-number-of-reviews-which-serve-as-a-proxy-for-bookings.-the-results-indicate-that-listings-active-on-the-platform-for-longer-periods-accumulate-more-reviews-with-each-additional-day-contributing-a-small-but-statistically-significant-increase.-higher-nightly-prices-are-also-positively-associated-with-review-counts-although-the-effect-size-is-modest.",
    "href": "projects/Project 1/MAA2.html#a-poisson-regression-model-was-used-to-analyze-how-airbnb-listing-characteristics-affect-the-number-of-reviews-which-serve-as-a-proxy-for-bookings.-the-results-indicate-that-listings-active-on-the-platform-for-longer-periods-accumulate-more-reviews-with-each-additional-day-contributing-a-small-but-statistically-significant-increase.-higher-nightly-prices-are-also-positively-associated-with-review-counts-although-the-effect-size-is-modest.",
    "title": "1 DATA",
    "section": "A Poisson regression model was used to analyze how Airbnb listing characteristics affect the number of reviews, which serve as a proxy for bookings. The results indicate that listings active on the platform for longer periods accumulate more reviews, with each additional day contributing a small but statistically significant increase. Higher nightly prices are also positively associated with review counts, although the effect size is modest.",
    "text": "A Poisson regression model was used to analyze how Airbnb listing characteristics affect the number of reviews, which serve as a proxy for bookings. The results indicate that listings active on the platform for longer periods accumulate more reviews, with each additional day contributing a small but statistically significant increase. Higher nightly prices are also positively associated with review counts, although the effect size is modest."
  },
  {
    "objectID": "projects/Project 1/MAA2.html#listings-with-more-bedrooms-receive-substantially-more-reviewseach-additional-bedroom-is-linked-to-a-12.7-increase-in-expected-bookings.-importantly-room-type-significantly-affects-performance-private-rooms-receive-approximately-55-fewer-reviews-than-entire-homes-or-apartments-suggesting-a-strong-guest-preference-for-full-property-rentals.",
    "href": "projects/Project 1/MAA2.html#listings-with-more-bedrooms-receive-substantially-more-reviewseach-additional-bedroom-is-linked-to-a-12.7-increase-in-expected-bookings.-importantly-room-type-significantly-affects-performance-private-rooms-receive-approximately-55-fewer-reviews-than-entire-homes-or-apartments-suggesting-a-strong-guest-preference-for-full-property-rentals.",
    "title": "1 DATA",
    "section": "Listings with more bedrooms receive substantially more reviews—each additional bedroom is linked to a 12.7% increase in expected bookings. Importantly, room type significantly affects performance: private rooms receive approximately 55% fewer reviews than entire homes or apartments, suggesting a strong guest preference for full-property rentals.",
    "text": "Listings with more bedrooms receive substantially more reviews—each additional bedroom is linked to a 12.7% increase in expected bookings. Importantly, room type significantly affects performance: private rooms receive approximately 55% fewer reviews than entire homes or apartments, suggesting a strong guest preference for full-property rentals."
  },
  {
    "objectID": "projects/Project 1/MAA2.html#furthermore-listings-with-the-instant-book-feature-perform-better-averaging-22-more-reviews-than-those-requiring-manual-approval.-this-highlights-the-benefit-of-reducing-booking-friction-for-potential-guests.",
    "href": "projects/Project 1/MAA2.html#furthermore-listings-with-the-instant-book-feature-perform-better-averaging-22-more-reviews-than-those-requiring-manual-approval.-this-highlights-the-benefit-of-reducing-booking-friction-for-potential-guests.",
    "title": "1 DATA",
    "section": "Furthermore, listings with the “instant book” feature perform better, averaging 22% more reviews than those requiring manual approval. This highlights the benefit of reducing booking friction for potential guests.",
    "text": "Furthermore, listings with the “instant book” feature perform better, averaging 22% more reviews than those requiring manual approval. This highlights the benefit of reducing booking friction for potential guests."
  },
  {
    "objectID": "projects/Project 1/MAA2.html#overall-the-analysis-suggests-that-to-increase-bookings-hosts-may-benefit-from-offering-entire-properties-enabling-instant-booking-and-enhancing-their-listings-over-time.-these-findings-can-help-guide-pricing-and-feature-decisions-to-improve-listing-visibility-and-appeal.",
    "href": "projects/Project 1/MAA2.html#overall-the-analysis-suggests-that-to-increase-bookings-hosts-may-benefit-from-offering-entire-properties-enabling-instant-booking-and-enhancing-their-listings-over-time.-these-findings-can-help-guide-pricing-and-feature-decisions-to-improve-listing-visibility-and-appeal.",
    "title": "1 DATA",
    "section": "Overall, the analysis suggests that to increase bookings, hosts may benefit from offering entire properties, enabling instant booking, and enhancing their listings over time. These findings can help guide pricing and feature decisions to improve listing visibility and appeal.",
    "text": "Overall, the analysis suggests that to increase bookings, hosts may benefit from offering entire properties, enabling instant booking, and enhancing their listings over time. These findings can help guide pricing and feature decisions to improve listing visibility and appeal."
  },
  {
    "objectID": "projects/Project 1/hw2_questions.html",
    "href": "projects/Project 1/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "projects/Project 1/hw2_questions.html#blueprinty-case-study",
    "href": "projects/Project 1/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "projects/Project 1/hw2_questions.html#airbnb-case-study",
    "href": "projects/Project 1/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided."
  },
  {
    "objectID": "hw3_questions.html",
    "href": "hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "hw3_questions.html#simulate-conjoint-data",
    "href": "hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))"
  },
  {
    "objectID": "hw3_questions.html#preparing-the-data-for-estimation",
    "href": "hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\ntodo: reshape and prep the data"
  },
  {
    "objectID": "hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\ntodo: Code up the log-likelihood function.\ntodo: Use optim() in R or scipy.optimize() in Python to find the MLEs for the 4 parameters (\\(\\beta_\\text{netflix}\\), \\(\\beta_\\text{prime}\\), \\(\\beta_\\text{ads}\\), \\(\\beta_\\text{price}\\)), as well as their standard errors (from the Hessian). For each parameter construct a 95% confidence interval."
  },
  {
    "objectID": "hw3_questions.html#estimation-via-bayesian-methods",
    "href": "hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\ntodo: code up a metropolis-hasting MCMC sampler of the posterior distribution. Take 11,000 steps and throw away the first 1,000, retaining the subsequent 10,000.\nhint: Use N(0,5) priors for the betas on the binary variables, and a N(0,1) prior for the price beta.\n_hint: instead of calculating post=lik*prior, you can work in the log-space and calculate log-post = log-lik + log-prior (this should enable you to re-use your log-likelihood function from the MLE section just above)_\nhint: King Markov (in the video) use a candidate distribution of a coin flip to decide whether to move left or right among his islands. Unlike King Markov, we have 4 dimensions (because we have 4 betas) and our dimensions are continuous. So, use a multivariate normal distribution to pospose the next location for the algorithm to move to. I recommend a MNV(mu, Sigma) where mu=c(0,0,0,0) and sigma has diagonal values c(0.05, 0.05, 0.05, 0.005) and zeros on the off-diagonal. Since this MVN has no covariances, you can sample each dimension independently (so 4 univariate normals instead of 1 multivariate normal), where the first 3 univariate normals are N(0,0.05) and the last one if N(0,0.005).\ntodo: for at least one of the 4 parameters, show the trace plot of the algorithm, as well as the histogram of the posterior distribution.\ntodo: report the 4 posterior means, standard deviations, and 95% credible intervals and compare them to your results from the Maximum Likelihood approach."
  },
  {
    "objectID": "hw3_questions.html#discussion",
    "href": "hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\ntodo: Suppose you did not simulate the data. What do you observe about the parameter estimates? What does \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) mean? Does it make sense that \\(\\beta_\\text{price}\\) is negative?\ntodo: At a high level, discuss what change you would need to make in order to simulate data from — and estimate the parameters of — a multi-level (aka random-parameter or hierarchical) model. This is the model we use to analyze “real world” conjoint data."
  },
  {
    "objectID": "hw4_questions.html",
    "href": "hw4_questions.html",
    "title": "Add Title",
    "section": "",
    "text": "todo: do two analyses. Do one of either 1a or 1b, AND one of either 2a or 2b."
  },
  {
    "objectID": "hw4_questions.html#a.-k-means",
    "href": "hw4_questions.html#a.-k-means",
    "title": "Add Title",
    "section": "1a. K-Means",
    "text": "1a. K-Means\ntodo: write your own code to implement the k-means algorithm. Make plots of the various steps the algorithm takes so you can “see” the algorithm working. Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables. Compare your results to the built-in kmeans function in R or Python.\ntodo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,…,7). What is the “right” number of clusters as suggested by these two metrics?\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this."
  },
  {
    "objectID": "hw4_questions.html#b.-latent-class-mnl",
    "href": "hw4_questions.html#b.-latent-class-mnl",
    "title": "Add Title",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\ntodo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57.\nThe data provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices in price-per-ounce (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current “wide” format into a “long” format.\ntodo: Fit the standard MNL model on these data. Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes.\ntodo: How many classes are suggested by the \\(BIC = -2*\\ell_n  + k*log(n)\\)? (where \\(\\ell_n\\) is the log-likelihood, \\(n\\) is the sample size, and \\(k\\) is the number of parameters.) The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate – akin to the adjusted R-squared for the linear regression model. Note, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model.\ntodo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC."
  },
  {
    "objectID": "hw4_questions.html#a.-k-nearest-neighbors",
    "href": "hw4_questions.html#a.-k-nearest-neighbors",
    "title": "Add Title",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\ntodo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm. The code generates a dataset with two features, x1 and x2, and a binary outcome variable y that is determined by whether x2 is above or below a wiggly boundary defined by a sin function.\n\n# gen data -----\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nx &lt;- cbind(x1, x2)\n\n# define a wiggly boundary\nboundary &lt;- sin(4*x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\ntodo: plot the data where the horizontal axis is x1, the vertical axis is x2, and the points are colored by the value of y. You may optionally draw the wiggly boundary.\ntodo: generate a test dataset with 100 points, using the same code as above but with a different seed.\ntodo: implement KNN by hand. Check you work with a built-in function – eg, class::knn() or caret::train(method=\"knn\") in R, or scikit-learn’s KNeighborsClassifier in Python.\ntodo: run your function for k=1,…,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?"
  },
  {
    "objectID": "hw4_questions.html#b.-key-drivers-analysis",
    "href": "hw4_questions.html#b.-key-drivers-analysis",
    "title": "Add Title",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations “by hand.”\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  },
  {
    "objectID": "MAA 3.html",
    "href": "MAA 3.html",
    "title": "Marketing Assignment 3",
    "section": "",
    "text": "import pandas as pd\nimport statsmodels.api as sm\n\n# Load the data\ndf = pd.read_csv(\"conjoint_data.csv\")\n\n# Step 1: One-hot encode categorical variables (drop first to avoid multicollinearity)\ndf_encoded = pd.get_dummies(df, columns=['brand', 'ad'], drop_first=True)\n\n# Step 2: Define the features (X) and target (y)\nX = df_encoded[['price', 'brand_N', 'brand_P', 'ad_Yes']].astype(float)  # Ensure numeric\ny = df_encoded['choice'].astype(int)  # Ensure binary integer outcome\n\n# Step 3: Add intercept\nX = sm.add_constant(X)\n\n# Step 4: Fit Multinomial Logit Model\nmodel = sm.MNLogit(y, X)\nresult = model.fit(method='newton', full_output=True, maxiter=100, disp=True)\n\n# Step 5: Print results\nprint(result.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.565003\n         Iterations 6\n                          MNLogit Regression Results                          \n==============================================================================\nDep. Variable:                 choice   No. Observations:                 3000\nModel:                        MNLogit   Df Residuals:                     2995\nMethod:                           MLE   Df Model:                            4\nDate:                Wed, 28 May 2025   Pseudo R-squ.:                  0.1123\nTime:                        14:06:22   Log-Likelihood:                -1695.0\nconverged:                       True   LL-Null:                       -1909.5\nCovariance Type:            nonrobust   LLR p-value:                 1.454e-91\n==============================================================================\n  choice=1       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.8818      0.132      6.680      0.000       0.623       1.141\nprice         -0.0901      0.006    -16.209      0.000      -0.101      -0.079\nbrand_N        0.8935      0.105      8.510      0.000       0.688       1.099\nbrand_P        0.4859      0.106      4.585      0.000       0.278       0.694\nad_Yes        -0.7489      0.085     -8.834      0.000      -0.915      -0.583\n=============================================================================="
  },
  {
    "objectID": "MAA 3.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "MAA 3.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Marketing Assignment 3",
    "section": "",
    "text": "import pandas as pd\nimport statsmodels.api as sm\n\n# Load the data\ndf = pd.read_csv(\"conjoint_data.csv\")\n\n# Step 1: One-hot encode categorical variables (drop first to avoid multicollinearity)\ndf_encoded = pd.get_dummies(df, columns=['brand', 'ad'], drop_first=True)\n\n# Step 2: Define the features (X) and target (y)\nX = df_encoded[['price', 'brand_N', 'brand_P', 'ad_Yes']].astype(float)  # Ensure numeric\ny = df_encoded['choice'].astype(int)  # Ensure binary integer outcome\n\n# Step 3: Add intercept\nX = sm.add_constant(X)\n\n# Step 4: Fit Multinomial Logit Model\nmodel = sm.MNLogit(y, X)\nresult = model.fit(method='newton', full_output=True, maxiter=100, disp=True)\n\n# Step 5: Print results\nprint(result.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.565003\n         Iterations 6\n                          MNLogit Regression Results                          \n==============================================================================\nDep. Variable:                 choice   No. Observations:                 3000\nModel:                        MNLogit   Df Residuals:                     2995\nMethod:                           MLE   Df Model:                            4\nDate:                Wed, 28 May 2025   Pseudo R-squ.:                  0.1123\nTime:                        14:06:22   Log-Likelihood:                -1695.0\nconverged:                       True   LL-Null:                       -1909.5\nCovariance Type:            nonrobust   LLR p-value:                 1.454e-91\n==============================================================================\n  choice=1       coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.8818      0.132      6.680      0.000       0.623       1.141\nprice         -0.0901      0.006    -16.209      0.000      -0.101      -0.079\nbrand_N        0.8935      0.105      8.510      0.000       0.688       1.099\nbrand_P        0.4859      0.106      4.585      0.000       0.278       0.694\nad_Yes        -0.7489      0.085     -8.834      0.000      -0.915      -0.583\n=============================================================================="
  },
  {
    "objectID": "MAA 3.html#simulate-conjoint-data",
    "href": "MAA 3.html#simulate-conjoint-data",
    "title": "Marketing Assignment 3",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\n\nimport numpy as np\nimport pandas as pd\nnp.random.seed(123)\n\nbrands = [\"N\", \"P\", \"H\"]               # N = Netflix, P = Prime, H = Hulu (baseline)\nads = [\"Yes\", \"No\"]                    # Ads present or not\nprices = np.arange(8, 33, 4)           # Price from $8 to $32 in $4 increments\n\nprofiles = pd.DataFrame([\n    (b, a, p) for b in brands for a in ads for p in prices\n], columns=[\"brand\", \"ad\", \"price\"])\n\nbrand_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}     # Reference: H = 0\nad_util = {\"Yes\": -0.8, \"No\": 0.0}              # Reference: Ad-free = 0\nprice_util = lambda p: -0.1 * p                 # Linear price penalty\n\nn_respondents = 100\nn_tasks = 10\nn_alts = 3\n\ndef simulate_respondent(resp_id):\n    respondent_data = []\n\n    for task_id in range(1, n_tasks + 1):\n        # Randomly sample 3 profiles (alternatives)\n        alts = profiles.sample(n=n_alts).copy()\n\n        # Compute deterministic part of utility\n        v = (\n            alts[\"brand\"].map(brand_util) +\n            alts[\"ad\"].map(ad_util) +\n            alts[\"price\"].apply(price_util)\n        )\n\n        # Add Gumbel-distributed random error\n        epsilon = -np.log(-np.log(np.random.rand(n_alts)))\n        u = v + epsilon\n\n        # Choose the alternative with the highest utility\n        alts[\"choice\"] = (u == u.max()).astype(int)\n\n        # Add respondent and task identifiers\n        alts[\"resp\"] = resp_id\n        alts[\"task\"] = task_id\n\n        # Append to list\n        respondent_data.append(alts)\n\n    return pd.concat(respondent_data)\n\nsimulated_data = pd.concat([\n    simulate_respondent(i) for i in range(1, n_respondents + 1)\n])\n\nsimulated_data = simulated_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]\n\n# Preview result\nprint(simulated_data.head())\n\n\n    resp  task brand  ad  price  choice\n27     1     1     P  No     32       0\n12     1     1     N  No     28       0\n11     1     1     N  No     24       1\n40     1     2     H  No     28       0\n35     1     2     H  No      8       1"
  },
  {
    "objectID": "MAA 3.html#preparing-the-data-for-estimation",
    "href": "MAA 3.html#preparing-the-data-for-estimation",
    "title": "Marketing Assignment 3",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\n\n\ndf = simulated_data.copy()\ndf_encoded = pd.get_dummies(df, columns=[\"brand\", \"ad\"], drop_first=True)\n\n# Step 3: Confirm your columns\n# You should now have: brand_N, brand_P, ad_Yes, price\nprint(df_encoded.head())\n\n# Step 4: Optional – sort by respondent and task (not necessary for model, but helpful)\ndf_encoded = df_encoded.sort_values(by=[\"resp\", \"task\"])\n\n# Step 5: Define X (predictors) and y (choice indicator)\nX_cols = ['price', 'brand_N', 'brand_P', 'ad_Yes']\nX = df_encoded[X_cols]\ny = df_encoded['choice']\n\n# Step 6: Add intercept\nimport statsmodels.api as sm\nX = sm.add_constant(X)\n\n# Ready for estimation\nprint(X.head())\nprint(y.head())\n\n    resp  task  price  choice  brand_N  brand_P  ad_Yes\n27     1     1     32       0    False     True   False\n12     1     1     28       0     True    False   False\n11     1     1     24       1     True    False   False\n40     1     2     28       0    False    False   False\n35     1     2      8       1    False    False   False\n    const  price  brand_N  brand_P  ad_Yes\n27    1.0     32    False     True   False\n12    1.0     28     True    False   False\n11    1.0     24     True    False   False\n40    1.0     28    False    False   False\n35    1.0      8    False    False   False\n27    0\n12    0\n11    1\n40    0\n35    1\nName: choice, dtype: int64"
  },
  {
    "objectID": "MAA 3.html#estimation-via-maximum-likelihood",
    "href": "MAA 3.html#estimation-via-maximum-likelihood",
    "title": "Marketing Assignment 3",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\nfrom scipy.optimize import minimize\nimport numpy as np\n\n# Ensure arrays are cleanly formatted\nX_np = np.asarray(X.to_numpy(), dtype=np.float64)\ny_np = np.asarray(y.to_numpy(), dtype=np.int64)\n\n# Number of alternatives per task\nJ = 3\n\n# Define the MNL log-likelihood function\ndef mnl_log_likelihood(beta):\n    beta = np.asarray(beta, dtype=np.float64)\n    utility = np.dot(X_np, beta).reshape(-1, J)\n    y_reshaped = y_np.reshape(-1, J)\n\n    # Apply stability trick for exponentiation\n    utility_adj = utility - utility.max(axis=1, keepdims=True)\n    exp_u = np.exp(utility_adj)\n    probs = exp_u / exp_u.sum(axis=1, keepdims=True)\n\n    # Pick only probabilities of chosen alternatives\n    chosen_probs = probs[y_reshaped == 1]\n    log_likelihood = np.sum(np.log(chosen_probs + 1e-12))  # avoid log(0)\n\n    return -log_likelihood  # Negative for minimization\n\n# Initial guess for beta\ninitial_beta = np.zeros(X_np.shape[1])\n\n# Run optimization\nresult = minimize(mnl_log_likelihood, initial_beta, method='BFGS')\n\n# Output\nprint(\"Optimization success:\", result.success)\nprint(\"Estimated beta coefficients:\", result.x)\nprint(\"Final log-likelihood:\", -result.fun)\n\nOptimization success: True\nEstimated beta coefficients: [ 0.         -0.09641815  1.05689172  0.47329576 -0.7723847 ]\nFinal log-likelihood: -863.5783346348348\n\n\n\nfrom scipy.optimize import minimize\n\n# Prepare data\nX_np = np.asarray(X.to_numpy(), dtype=np.float64)\ny_np = np.asarray(y.to_numpy(), dtype=np.int64)\nJ = 3  # Alternatives per task\n\n# Log-likelihood function\ndef mnl_log_likelihood(beta):\n    beta = np.asarray(beta, dtype=np.float64)\n    utility = np.dot(X_np, beta).reshape(-1, J)\n    y_reshaped = y_np.reshape(-1, J)\n\n    utility_adj = utility - utility.max(axis=1, keepdims=True)  # for stability\n    exp_u = np.exp(utility_adj)\n    probs = exp_u / exp_u.sum(axis=1, keepdims=True)\n\n    chosen_probs = probs[y_reshaped == 1]\n    log_likelihood = np.sum(np.log(chosen_probs + 1e-12))\n    return -log_likelihood  # minimize negative log-likelihood\n\n# Initial guess\ninitial_beta = np.zeros(X_np.shape[1])\n\n# Run MLE\nresult = minimize(mnl_log_likelihood, initial_beta, method='BFGS')\n\n# Extract estimates and Hessian\nbeta_hat = result.x\nhessian_inv = result.hess_inv  # Inverse Hessian = covariance matrix\n\n# Calculate standard errors\nstandard_errors = np.sqrt(np.diag(hessian_inv))\n\n# Construct 95% confidence intervals\nz = 1.96\nci_lower = beta_hat - z * standard_errors\nci_upper = beta_hat + z * standard_errors\n\n# Display results in a DataFrame\nsummary = pd.DataFrame({\n    'Parameter': X.columns,\n    'Estimate': beta_hat,\n    'Std. Error': standard_errors,\n    'CI Lower (95%)': ci_lower,\n    'CI Upper (95%)': ci_upper\n})\n\nprint(summary)\n\n  Parameter  Estimate  Std. Error  CI Lower (95%)  CI Upper (95%)\n0     const  0.000000    1.000000       -1.960000        1.960000\n1     price -0.096418    0.006056       -0.108288       -0.084549\n2   brand_N  1.056892    0.096609        0.867539        1.246245\n3   brand_P  0.473296    0.091216        0.294513        0.652079\n4    ad_Yes -0.772385    0.094500       -0.957604       -0.587165"
  },
  {
    "objectID": "MAA 3.html#estimation-via-bayesian-methods",
    "href": "MAA 3.html#estimation-via-bayesian-methods",
    "title": "Marketing Assignment 3",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\n\nfrom scipy.special import logsumexp\nimport matplotlib.pyplot as plt\n\n# Step 1: Use only actual variables (no intercept)\nX_bayes = X.drop(columns=['const'])\nX_np = np.asarray(X_bayes.to_numpy(), dtype=np.float64)\ny_np = np.asarray(y.to_numpy(), dtype=np.int64)\nJ = 3  # 3 alternatives per task\n\n# Step 2: Log-likelihood function\ndef mnl_log_likelihood(beta):\n    beta = np.asarray(beta, dtype=np.float64)\n    utility = np.dot(X_np, beta).reshape(-1, J)\n    y_reshaped = y_np.reshape(-1, J)\n    \n    # Stability fix\n    log_prob = utility - logsumexp(utility, axis=1, keepdims=True)\n    chosen_log_prob = log_prob[y_reshaped == 1]\n    \n    return np.sum(chosen_log_prob)\n\n# Step 3: Log-prior function\ndef log_prior(beta):\n    # N(0, 5^2) for first 3 (binary vars), N(0, 1^2) for price\n    return -0.5 * ((beta[0:3]**2 / 25).sum() + (beta[3]**2))\n\n# Step 4: Log-posterior function\ndef log_posterior(beta):\n    return mnl_log_likelihood(beta) + log_prior(beta)\n\n# Step 5: Metropolis-Hastings MCMC\nn_steps = 11000\nburn_in = 1000\nsamples = []\n\ncurrent_beta = np.zeros(4)\ncurrent_log_post = log_posterior(current_beta)\n\nproposal_sds = np.array([0.05, 0.05, 0.05, 0.005])\n\nfor step in range(n_steps):\n    proposal = current_beta + np.random.normal(0, proposal_sds)\n    proposal_log_post = log_posterior(proposal)\n    \n    log_accept_ratio = proposal_log_post - current_log_post\n    if np.log(np.random.rand()) &lt; log_accept_ratio:\n        current_beta = proposal\n        current_log_post = proposal_log_post\n    \n    samples.append(current_beta.copy())\n\n# Step 6: Analyze posterior\nsamples_np = np.array(samples)\nposterior_samples = samples_np[burn_in:]  # discard burn-in\n\nposterior_means = posterior_samples.mean(axis=0)\nlower_bounds = np.percentile(posterior_samples, 2.5, axis=0)\nupper_bounds = np.percentile(posterior_samples, 97.5, axis=0)\n\nposterior_summary = pd.DataFrame({\n    'Parameter': X_bayes.columns,\n    'Posterior Mean': posterior_means,\n    'CI Lower (95%)': lower_bounds,\n    'CI Upper (95%)': upper_bounds\n})\n\n# Step 7: Display summary\nprint(posterior_summary)\n\n# Step 8: Optional – Trace plots\nfig, axes = plt.subplots(4, 1, figsize=(10, 8), sharex=True)\nparam_names = X_bayes.columns\n\nfor i in range(4):\n    axes[i].plot(posterior_samples[:, i], color='black', linewidth=0.5)\n    axes[i].set_title(f'Trace plot: {param_names[i]}')\n    axes[i].axhline(posterior_means[i], color='red', linestyle='--')\n\nplt.xlabel(\"Iteration\")\nplt.tight_layout()\nplt.show()\n\n  Parameter  Posterior Mean  CI Lower (95%)  CI Upper (95%)\n0     price       -0.095042       -0.107740       -0.083538\n1   brand_N        1.023532        0.790482        1.267125\n2   brand_P        0.467587        0.221159        0.702555\n3    ad_Yes       -0.559417       -0.837536       -0.184598\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\n# Step 1: Select 'price' parameter samples from posterior\nparam_index = list(X_bayes.columns).index('price')\nparam_samples = posterior_samples[:, param_index]\n\n# Step 2: Plot trace and histogram\nfig, axes = plt.subplots(2, 1, figsize=(10, 6))\n\n# Trace plot\naxes[0].plot(param_samples, color='blue', linewidth=0.5)\naxes[0].set_title(\"Trace Plot for 'price'\")\naxes[0].set_ylabel(\"Value\")\naxes[0].axhline(param_samples.mean(), color='red', linestyle='--', label='Posterior Mean')\naxes[0].legend()\n\n# Histogram\naxes[1].hist(param_samples, bins=40, color='skyblue', edgecolor='black', density=True)\naxes[1].set_title(\"Posterior Distribution of 'price'\")\naxes[1].set_xlabel(\"Value\")\naxes[1].set_ylabel(\"Density\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n# --- Step 1: MLE Summary Table ---\n# Calculate 95% confidence intervals using standard normal quantile\nz = 1.96\nci_lower = beta_hat - z * standard_errors\nci_upper = beta_hat + z * standard_errors\n\nsummary_df = pd.DataFrame({\n    'Parameter': X.columns,\n    'Estimate': beta_hat,\n    'Std. Error': standard_errors,\n    'CI Lower (95%)': ci_lower,\n    'CI Upper (95%)': ci_upper\n})\n\n# --- Step 2: Bayesian Posterior Statistics ---\nposterior_std = posterior_samples.std(axis=0)\n\n# --- Step 3: Match MLE to Bayesian parameter set (exclude intercept) ---\nmle_summary = summary_df.set_index(\"Parameter\").loc[X_bayes.columns]\n\n# --- Step 4: Final Comparison Table ---\ncomparison_df = pd.DataFrame({\n    \"Parameter\": X_bayes.columns,\n    \"Bayesian Mean\": posterior_means,\n    \"Bayesian Std\": posterior_std,\n    \"Bayesian CI Lower (95%)\": lower_bounds,\n    \"Bayesian CI Upper (95%)\": upper_bounds,\n    \"MLE Estimate\": mle_summary[\"Estimate\"].values,\n    \"MLE Std. Error\": mle_summary[\"Std. Error\"].values,\n    \"MLE CI Lower (95%)\": mle_summary[\"CI Lower (95%)\"].values,\n    \"MLE CI Upper (95%)\": mle_summary[\"CI Upper (95%)\"].values\n})\n\n# --- Step 5: Display Table ---\nprint(comparison_df)\n\n  Parameter  Bayesian Mean  Bayesian Std  Bayesian CI Lower (95%)  \\\n0     price      -0.095042      0.006200                -0.107740   \n1   brand_N       1.023532      0.124153                 0.790482   \n2   brand_P       0.467587      0.123393                 0.221159   \n3    ad_Yes      -0.559417      0.175565                -0.837536   \n\n   Bayesian CI Upper (95%)  MLE Estimate  MLE Std. Error  MLE CI Lower (95%)  \\\n0                -0.083538     -0.096418        0.006056           -0.108288   \n1                 1.267125      1.056892        0.096609            0.867539   \n2                 0.702555      0.473296        0.091216            0.294513   \n3                -0.184598     -0.772385        0.094500           -0.957604   \n\n   MLE CI Upper (95%)  \n0           -0.084549  \n1            1.246245  \n2            0.652079  \n3           -0.587165"
  },
  {
    "objectID": "MAA 3.html#discussion",
    "href": "MAA 3.html#discussion",
    "title": "Marketing Assignment 3",
    "section": "6. Discussion",
    "text": "6. Discussion\nEven if we didn’t know this was simulated data, the model results still tell a pretty clear story. First, we can see that Netflix has the highest utility—its coefficient is larger than Amazon Prime’s. That means, all else being equal, people are more likely to choose Netflix over Prime, and both are preferred over Hulu (which was the baseline in our model). This fits with what we might expect in real life, given Netflix’s strong brand and large content library.\nThe fact that βₙₑₜ𝒻ₗᵢ𝓍 &gt; βₚᵣᵢₘₑ simply means that switching from Prime to Netflix increases the chance of a consumer choosing that option—it gives more “utility” to the user in the model’s eyes.\nAlso, price has a negative effect, which totally makes sense. As the subscription price goes up, the probability of someone picking that service goes down. This reflects basic consumer behavior: people generally prefer cheaper options when everything else is the same.\nSo overall, the results look reasonable and match how we’d expect people to behave when choosing between streaming platforms.\nIf we wanted to make our model more realistic—like what we’d use for real-world conjoint analysis—we’d need to move to a multi-level model (also called a hierarchical or random-parameter model).\nRight now, our model assumes that everyone thinks the same way—that all consumers value Netflix, ads, and price the same. But that’s not how people actually behave. Some might love Netflix, others might prefer Prime, and some may be more price-sensitive than others.\nIn a multi-level model, we fix that by giving each person their own set of preferences. Instead of one set of beta values, we assume each individual has their own, drawn from a larger population distribution. So for example, my β values might come from a normal distribution centered around the average consumer, but they still reflect my personal tastes.\nTo simulate this kind of data, we’d just draw a different set of betas for each respondent and then generate their choices based on those personal betas. And to estimate the model, we’d no longer just estimate one set of coefficients—we’d estimate the distribution those betas come from (like the mean and variance), and maybe even estimate each person’s betas too.\nIt’s more complex, but it’s also more powerful, because it captures the fact that people are different. That’s why hierarchical models are the go-to for analyzing real conjoint survey data."
  },
  {
    "objectID": "MAA4.html",
    "href": "MAA4.html",
    "title": "Marketing Analytics part 4",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load dataset\ndf = pd.read_csv('palmer_penguins.csv')\n\n# Use only the columns needed\ndf = df[['bill_length_mm', 'flipper_length_mm']].dropna()\nX = df.to_numpy()\n\n\ndef initialize_centroids(X, k):\n    np.random.seed(42)\n    indices = np.random.choice(len(X), k, replace=False)\n    return X[indices]\n\ndef assign_clusters(X, centroids):\n    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n    return np.argmin(distances, axis=1)\n\ndef update_centroids(X, labels, k):\n    return np.array([X[labels == i].mean(axis=0) for i in range(k)])\n\ndef kmeans_custom(X, k, max_iters=10):\n    centroids = initialize_centroids(X, k)\n    all_centroids = [centroids.copy()]\n    \n    for _ in range(max_iters):\n        labels = assign_clusters(X, centroids)\n        centroids = update_centroids(X, labels, k)\n        all_centroids.append(centroids.copy())\n    \n    return labels, all_centroids\n\n\ndef plot_iterations(X, all_centroids):\n    for i, centroids in enumerate(all_centroids[:-1]):\n        labels = assign_clusters(X, centroids)\n        plt.figure(figsize=(6, 4))\n        for cluster in range(len(centroids)):\n            cluster_points = X[labels == cluster]\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster}')\n        plt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x', s=100, label='Centroids')\n        plt.title(f'Iteration {i}')\n        plt.xlabel(\"Bill Length (mm)\")\n        plt.ylabel(\"Flipper Length (mm)\")\n        plt.legend()\n        plt.show()\n\n\nk = 3\nlabels_custom, all_centroids = kmeans_custom(X, k)\nplot_iterations(X, all_centroids)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import KMeans\n\nkmeans_builtin = KMeans(n_clusters=3, random_state=42)\nlabels_builtin = kmeans_builtin.fit_predict(X)\n\n# Plot comparison\nplt.figure(figsize=(6, 4))\nplt.scatter(X[:, 0], X[:, 1], c=labels_builtin, cmap='viridis')\nplt.scatter(kmeans_builtin.cluster_centers_[:, 0], kmeans_builtin.cluster_centers_[:, 1], \n            c='red', marker='x', s=100, label='Built-in Centroids')\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.title(\"sklearn KMeans Result\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nOur custom K-Means gave similar results to sklearn, with only minor differences due to random starting points. Watching the centroids shift and points reassign step-by-step made the algorithm much easier to understand\n\n\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\n\n# Data already loaded and cleaned:\n# df = df[['bill_length_mm', 'flipper_length_mm']].dropna()\n# X = df.to_numpy()\n\n\nwcss = []\nsilhouette_scores = []\nK_range = range(2, 8)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    labels = kmeans.fit_predict(X)\n    \n    wcss.append(kmeans.inertia_)  # Within-Cluster Sum of Squares\n    silhouette_scores.append(silhouette_score(X, labels))\n\n\nplt.figure(figsize=(12, 5))\n\n# Plot WCSS\nplt.subplot(1, 2, 1)\nplt.plot(K_range, wcss, marker='o')\nplt.title('Within-Cluster Sum of Squares (WCSS)')\nplt.xlabel('Number of clusters (k)')\nplt.ylabel('WCSS')\n\n# Plot Silhouette Score\nplt.subplot(1, 2, 2)\nplt.plot(K_range, silhouette_scores, marker='o')\nplt.title('Silhouette Score')\nplt.xlabel('Number of clusters (k)')\nplt.ylabel('Silhouette Score')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nBoth metrics suggest that k = 3 is the optimal number of clusters. The WCSS plot shows an elbow at 3, and the silhouette score is highest at k = 2, but remains strong at k = 3, indicating well-separated clusters."
  },
  {
    "objectID": "MAA4.html#write-your-own-code-to-implement-the-k-means-algorithm.-make-plots-of-the-various-steps-the-algorithm-takes-so-you-can-see-the-algorithm-working.-test-your-algorithm-on-the-palmer-penguins-dataset-specifically-using-the-bill-length-and-flipper-length-variables.-compare-your-results-to-the-built-in-kmeans-function-in-r-or-python._",
    "href": "MAA4.html#write-your-own-code-to-implement-the-k-means-algorithm.-make-plots-of-the-various-steps-the-algorithm-takes-so-you-can-see-the-algorithm-working.-test-your-algorithm-on-the-palmer-penguins-dataset-specifically-using-the-bill-length-and-flipper-length-variables.-compare-your-results-to-the-built-in-kmeans-function-in-r-or-python._",
    "title": "Marketing Analytics part 4",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Load dataset\ndf = pd.read_csv('palmer_penguins.csv')\n\n# Use only the columns needed\ndf = df[['bill_length_mm', 'flipper_length_mm']].dropna()\nX = df.to_numpy()\n\n\ndef initialize_centroids(X, k):\n    np.random.seed(42)\n    indices = np.random.choice(len(X), k, replace=False)\n    return X[indices]\n\ndef assign_clusters(X, centroids):\n    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n    return np.argmin(distances, axis=1)\n\ndef update_centroids(X, labels, k):\n    return np.array([X[labels == i].mean(axis=0) for i in range(k)])\n\ndef kmeans_custom(X, k, max_iters=10):\n    centroids = initialize_centroids(X, k)\n    all_centroids = [centroids.copy()]\n    \n    for _ in range(max_iters):\n        labels = assign_clusters(X, centroids)\n        centroids = update_centroids(X, labels, k)\n        all_centroids.append(centroids.copy())\n    \n    return labels, all_centroids\n\n\ndef plot_iterations(X, all_centroids):\n    for i, centroids in enumerate(all_centroids[:-1]):\n        labels = assign_clusters(X, centroids)\n        plt.figure(figsize=(6, 4))\n        for cluster in range(len(centroids)):\n            cluster_points = X[labels == cluster]\n            plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster}')\n        plt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x', s=100, label='Centroids')\n        plt.title(f'Iteration {i}')\n        plt.xlabel(\"Bill Length (mm)\")\n        plt.ylabel(\"Flipper Length (mm)\")\n        plt.legend()\n        plt.show()\n\n\nk = 3\nlabels_custom, all_centroids = kmeans_custom(X, k)\nplot_iterations(X, all_centroids)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import KMeans\n\nkmeans_builtin = KMeans(n_clusters=3, random_state=42)\nlabels_builtin = kmeans_builtin.fit_predict(X)\n\n# Plot comparison\nplt.figure(figsize=(6, 4))\nplt.scatter(X[:, 0], X[:, 1], c=labels_builtin, cmap='viridis')\nplt.scatter(kmeans_builtin.cluster_centers_[:, 0], kmeans_builtin.cluster_centers_[:, 1], \n            c='red', marker='x', s=100, label='Built-in Centroids')\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.title(\"sklearn KMeans Result\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nOur custom K-Means gave similar results to sklearn, with only minor differences due to random starting points. Watching the centroids shift and points reassign step-by-step made the algorithm much easier to understand"
  },
  {
    "objectID": "MAA4.html#calculate-both-the-within-cluster-sum-of-squares-and-silhouette-scores-you-can-use-built-in-functions-to-do-so-and-plot-the-results-for-various-numbers-of-clusters-ie-k237.-what-is-the-right-number-of-clusters-as-suggested-by-these-two-metrics_",
    "href": "MAA4.html#calculate-both-the-within-cluster-sum-of-squares-and-silhouette-scores-you-can-use-built-in-functions-to-do-so-and-plot-the-results-for-various-numbers-of-clusters-ie-k237.-what-is-the-right-number-of-clusters-as-suggested-by-these-two-metrics_",
    "title": "Marketing Analytics part 4",
    "section": "",
    "text": "from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nimport matplotlib.pyplot as plt\n\n# Data already loaded and cleaned:\n# df = df[['bill_length_mm', 'flipper_length_mm']].dropna()\n# X = df.to_numpy()\n\n\nwcss = []\nsilhouette_scores = []\nK_range = range(2, 8)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    labels = kmeans.fit_predict(X)\n    \n    wcss.append(kmeans.inertia_)  # Within-Cluster Sum of Squares\n    silhouette_scores.append(silhouette_score(X, labels))\n\n\nplt.figure(figsize=(12, 5))\n\n# Plot WCSS\nplt.subplot(1, 2, 1)\nplt.plot(K_range, wcss, marker='o')\nplt.title('Within-Cluster Sum of Squares (WCSS)')\nplt.xlabel('Number of clusters (k)')\nplt.ylabel('WCSS')\n\n# Plot Silhouette Score\nplt.subplot(1, 2, 2)\nplt.plot(K_range, silhouette_scores, marker='o')\nplt.title('Silhouette Score')\nplt.xlabel('Number of clusters (k)')\nplt.ylabel('Silhouette Score')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nBoth metrics suggest that k = 3 is the optimal number of clusters. The WCSS plot shows an elbow at 3, and the silhouette score is highest at k = 2, but remains strong at k = 3, indicating well-separated clusters."
  }
]